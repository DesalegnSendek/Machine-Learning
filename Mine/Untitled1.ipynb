{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abdd765-a27f-4841-8cfb-68adf15c41b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd8a678-96e4-4017-b790-42feb269cb54",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV data into a DataFrame\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print a summary of the data (column names, data types, non-null values, etc.)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m data\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:586\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as snsÂ \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV data into a DataFrame\n",
    "data = pd.read_csv(\"Data.csv\")\n",
    "\n",
    "# Print a summary of the data (column names, data types, non-null values, etc.)\n",
    "data.info()\n",
    "\n",
    "# Drop rows with missing values (inplace=True modifies the DataFrame)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Print the summary again to see how missing values were handled\n",
    "data.info()\n",
    "\n",
    "# Import train_test_split from scikit-learn for splitting data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) from the target variable (Y)\n",
    "X = data.drop([\"median_house_value\"], axis=1)\n",
    "Y = data[\"median_house_value\"]\n",
    "\n",
    "# Split data into training and testing sets (80% for training, 20% for testing)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Combine training features and target variable back into a DataFrame\n",
    "train_data = X_train.join(Y_train)\n",
    "\n",
    "# Create dummy variables for the categorical feature \"ocean_proximity\"\n",
    "# Convert them to integers for compatibility with some machine learning models\n",
    "dummy_ocean_proximity = pd.get_dummies(train_data.ocean_proximity).astype(int)\n",
    "\n",
    "# Join the dummy variables back to the training data\n",
    "train_data = train_data.join(dummy_ocean_proximity)\n",
    "\n",
    "# Drop the original categorical feature \"ocean_proximity\" since it's now encoded in dummy variables\n",
    "train_data = train_data.drop(['ocean_proximity'],axis=1)\n",
    "\n",
    "# Visualize the distribution of features using histograms\n",
    "train_data.hist(figsize=(15,8))\n",
    "\n",
    "# Calculate the correlation matrix to see how features are related to each other\n",
    "train_data.corr()\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix (commented out, using seaborn heatmap instead)\n",
    "# plt.figure(figsize=(15,8))\n",
    "# sns.heatmap(train_data.corr(), annot=True, cmap=\"YlGnBu\")\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix using seaborn\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(train_data.corr(), annot=True, fmt=\".2f\", cmap=\"RdBu_r\")  # Adjust \".2f\" for desired decimal places\n",
    "plt.show()\n",
    "\n",
    "# Apply logarithmic transformation to some features to handle skewness\n",
    "train_data['total_rooms'] = np.log(train_data['total_rooms']+1)\n",
    "train_data['total_bedrooms'] = np.log(train_data['total_bedrooms']+1)\n",
    "train_data['population'] = np.log(train_data['population']+1)\n",
    "train_data['households'] = np.log(train_data['households']+1)\n",
    "\n",
    "# Visualize the distribution of features after transformation (histograms)\n",
    "train_data.hist(figsize=(15,8))\n",
    "\n",
    "# Create another heatmap to see how correlations changed after transformation\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(train_data.corr(),annot=True,cmap=\"RdBu_r\")\n",
    "\n",
    "# Create a scatterplot to visualize the relationship between latitude, longitude, and median house value\n",
    "sns.scatterplot(x='latitude',y='longitude',data=train_data, hue='median_house_value', palette='coolwarm')\n",
    "\n",
    "# Feature engineering: Create new features based on existing ones\n",
    "train_data['bedroom_ratio'] = train_data['total_bedrooms'] / train_data['total_rooms']\n",
    "train_data['household_rooms'] = train_data['total_rooms'] / train_data['households']\n",
    "\n",
    "# Create another heatmap to see how correlations changed after feature engineering\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(train_data.corr(), annot=True, cmap=\"RdBu_r\")\n",
    "\n",
    "# Import LinearRegression from scikit-learn for linear regression modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Separate features from target variable again for training\n",
    "X_train, Y_train = train_data.drop(['median_house_value'], axis=1), train_data['median_house_value']\n",
    "\n",
    "\n",
    "# Create a linear regression model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Train the model on the prepared training data\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the unseen test data using the trained model\n",
    "predictions = reg.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error (MSE)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"Mean Squared Error (MSE) on test data:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(Y_test, predictions)\n",
    "print(\"R-squared on test data:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0455cf-a059-4094-b40a-9d775e7b49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d8719-1635-4229-8e48-a99063f432ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9940f-3f00-4aac-ab80-0ff66b9bae49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
